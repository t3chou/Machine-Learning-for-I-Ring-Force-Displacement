---
description: In this page, I will demonstrate the model training process.
---

# 7. Model Training

## Model selection

In this phase, we'll opt for seven machine learning algorithm that are suitable for this classification question.

## Random Forest Classifier

Training process and test some prediction, and here is the illustration of this algorithm.

![](<.gitbook/assets/image (1).png>)

{% tabs %}
{% tab title="Python" %}
```python
from sklearn.ensemble import RandomForestClassifier
classifier1 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier1.fit(X_train, y_train)
```
{% endtab %}
{% endtabs %}

```python
y_pred1 = classifier1.predict(X_test)
print(np.concatenate((y_pred1.reshape(len(y_pred1),1), y_test.reshape(len(y_test),1)),1))
```

## Decision Tree Classifier

Training process and test some prediction, and here is the illustration of this algorithm.

![](<.gitbook/assets/image (14).png>)

{% tabs %}
{% tab title="Python" %}
```python
from sklearn.tree import DecisionTreeClassifier
classifier2 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier2.fit(X_train, y_train)
```
{% endtab %}
{% endtabs %}

```python
y_pred2 = classifier2.predict(X_test)
print(np.concatenate((y_pred2.reshape(len(y_pred2),1), y_test.reshape(len(y_test),1)),1))
```

## Gaussian Naive Bayes

Training process and test some prediction, and here is the illustration of this algorithm.

![](<.gitbook/assets/image (31).png>)

{% tabs %}
{% tab title="Python" %}
```python
from sklearn.naive_bayes import GaussianNB
classifier3 = GaussianNB()
classifier3.fit(X_train, y_train)
```
{% endtab %}
{% endtabs %}

```python
y_pred3 = classifier3.predict(X_test)
print(np.concatenate((y_pred3.reshape(len(y_pred3),1), y_test.reshape(len(y_test),1)),1))
```

## Support Vector Classification 1

Training process and test some prediction, and here is the illustration of this algorithm.

![](<.gitbook/assets/image (48).png>)

{% tabs %}
{% tab title="Python" %}
```python
from sklearn.svm import SVC
classifier4 = SVC(kernel = 'rbf', random_state = 0)
classifier4.fit(X_train, y_train)
```
{% endtab %}
{% endtabs %}

```python
y_pred4 = classifier4.predict(X_test)
print(np.concatenate((y_pred4.reshape(len(y_pred4),1), y_test.reshape(len(y_test),1)),1))
```

## Support Vector Classification 2

Training process and test some prediction, and here is the illustration of this algorithm.

![](<.gitbook/assets/image (26).png>)

{% tabs %}
{% tab title="Python" %}
```python
from sklearn.svm import SVC
classifier5 = SVC(kernel = 'linear', random_state = 0)
classifier5.fit(X_train, y_train)
```
{% endtab %}
{% endtabs %}

```python
y_pred5 = classifier5.predict(X_test)
print(np.concatenate((y_pred5.reshape(len(y_pred5),1), y_test.reshape(len(y_test),1)),1))
```

## K Neighbors Classifier

Training process and test some prediction, and here is the illustration of this algorithm.

![](<.gitbook/assets/image (24).png>)

{% tabs %}
{% tab title="Python" %}
```python
from sklearn.neighbors import KNeighborsClassifier

classifier6 = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) #連接點數為5

classifier6.fit(X_train, y_train) #開始訓練KNN模型
```
{% endtab %}
{% endtabs %}

```python
y_pred6 = classifier6.predict(X_test)
print(np.concatenate((y_pred6.reshape(len(y_pred6),1), y_test.reshape(len(y_test),1)),1))
```

## Logistic Regression

Training process and test some prediction, and here is the illustration of this algorithm.

![](<.gitbook/assets/image (5).png>)

{% tabs %}
{% tab title="Python" %}
```python
from sklearn.linear_model import LogisticRegression
classifier7 = LogisticRegression(random_state = 0)
classifier7.fit(X_train, y_train)
```
{% endtab %}
{% endtabs %}

```python
y_pred7 = classifier7.predict(X_test)
print(np.concatenate((y_pred7.reshape(len(y_pred7),1), y_test.reshape(len(y_test),1)),1))
```

## Ensemble Learning

In order to make the prediction better, we can adopt ensemble learning to boost the accuracy.

![](<.gitbook/assets/image (33).png>)

{% tabs %}
{% tab title="Python" %}
```python
newlist = []
afterNewList = []
for i in range(len(y_pred1)):
    newlist.append(y_pred1[i]+y_pred2[i]+y_pred3[i]+y_pred4[i]+y_pred5[i]+y_pred6[i]+y_pred7[i])
for i in range(len(newlist)):
    if(newlist[i]>=3):
        afterNewList.append(1)
    if(newlist[i]<3):
        afterNewList.append(0)
```
{% endtab %}
{% endtabs %}

```python
y_pred1[0]+y_pred2[0]+y_pred3[0]+y_pred4[0]+y_pred5[0]+y_pred6[0]+y_pred7[0]p
```

#### you can see that after ensemble learning has become 0 and 1 type again.

```
newlist
```

```
afterNewList
```

## Store Models

```python
for i in range(1,16):
    for j in range(1,8):
        print('C:/Users/choubrianty/Downloads/Ensemble/batch_'+str(i)+'/'+str(j))
```

Here is the overview of all model and scaler storage, which was trained for 15 batches, and then store them down.

![](<.gitbook/assets/image (19).png>)

There are 7 models and 1 scaler in one folder.

![](<.gitbook/assets/image (20).png>)
